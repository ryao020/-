# Deep learning visual attack

## Introduce

This is a project that uses python and some popular libraries and frameworks, such as numpy, pandas, matplotlib, tensorflow, pytorch, gym, etc., to implement white-box and black-box attacks on deep learning visual models.

## Purpose

The purpose of this project is to learn and practice how to generate adversarial examples, that is, images or videos that can fool deep learning models with minor modifications but are imperceptible to the naked eye. The project also aims to evaluate the effectiveness of different attack methods, such as FGSM, MI-FGSM, NI-FGSM, etc., as well as explore some novel methods to improve the transferability of adversarial examples, such as ensemble training, data enhancement, meta-learning, etc.

## Method

This project uses reinforcement learning (DRL) as the deep learning vision model and selects one game environments Cartpole as the vision tasks. This project uses different DRL algorithms (Deep Q-Learning, Policy Gradients) to train DRL agents, and uses white-box attack methods (FGSM, MI-FGSM, NI-FGSM) to generate adversarial samples. The purpose is to test the adversarial samples in different Transferability between algorithms and strategies. The project also tried some novel methods to improve the transferability of adversarial examples, such as using better optimization, ensemble training, data augmentation, meta-learning, etc.

## Result

This project shows that adversarial examples can successfully fool a DRL agent and degrade its performance, and that different attack methods have different success rates, transferability, and perturbation sizes. The project also shows some novel ways to improve the transferability of adversarial examples, but there are also some limitations and challenges.

## Analyze

This project analyzes the advantages and disadvantages of different attack methods, as well as the possible causes and effects of attack results. The project also discusses some interesting practical scenarios to apply attacks, such as autonomous driving, drones, etc., as well as some possible methods to mitigate adversarial attacks, such as adversarial training, detection, etc.

## Conclusion

The project concluded that deep learning vision models are vulnerable to adversarial attacks, and generating and transferring adversarial examples is a challenging and attractive topic. The project also raises some future directions and open questions for further research and exploration.
